{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the running code is saved on google collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -qy numpy==1.22.3 matplotlib==3.5.1 tensorflow==2.9.0 skillsnetwork==0.20.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the Required Libraries\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(f\"tensorflow version: {tf.__version__}\")\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input,Conv2DTranspose,BatchNormalization,ReLU,Conv2D,LeakyReLU\n",
    "\n",
    "\n",
    "from IPython import display\n",
    "import skillsnetwork\n",
    "print(f\"skillsnetwork version: {skillsnetwork.__version__}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from pathlib import Path\n",
    "import imghdr\n",
    "\n",
    "import time\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_array(X,title=\"\"):\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = (20,20) \n",
    "\n",
    "    for i,x in enumerate(X[0:5]):\n",
    "        x=x.numpy()\n",
    "        max_=x.max()\n",
    "        min_=x.min()\n",
    "        xnew=np.uint(255*(x-min_)/(max_-min_))\n",
    "        plt.subplot(1,5,i+1)\n",
    "        plt.imshow(xnew)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module6/cartoon_20000.zip\"\n",
    "await skillsnetwork.prepare(dataset_url, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we declare some properties of our images, including image height, image width, and batch size.\n",
    "img_height, img_width, batch_size=64,64,128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(directory='cartoon_20000', # change directory to 'cartoon_data' if you use the full dataset\n",
    "                                                       image_size=(img_height, img_width),\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       label_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The train_ds we defined is a tf.data.Dataset that yields batches of images with image_size = (64, 64) from the directory specified or subdirectories\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.experimental.preprocessing.Rescaling(scale= 1./127.5, offset=-1)\n",
    "normalized_ds = train_ds.map(lambda x: normalization_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[x for x in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_array(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building The Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    \n",
    "    model=Sequential()\n",
    "    \n",
    "    # input is latent vector of 100 dimensions\n",
    "    model.add(Input(shape=(1, 1, 100), name='input_layer'))\n",
    "    \n",
    "    # Block 1 dimensionality of the output space  64 * 8\n",
    "    model.add(Conv2DTranspose(64 * 8, kernel_size=4, strides= 4, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_transpose_1'))\n",
    "    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_1'))\n",
    "    model.add(ReLU(name='relu_1'))\n",
    "\n",
    "    # Block 2: input is 4 x 4 x (64 * 8)\n",
    "    model.add(Conv2DTranspose(64 * 4, kernel_size=4, strides= 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_transpose_2'))\n",
    "    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_2'))\n",
    "    model.add(ReLU(name='relu_2'))\n",
    "\n",
    "    # Block 3: input is 8 x 8 x (64 * 4)\n",
    "    model.add(Conv2DTranspose(64 * 2, kernel_size=4,strides=  2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_transpose_3'))\n",
    "    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8,  center=1.0, scale=0.02, name='bn_3'))\n",
    "    model.add(ReLU(name='relu_3'))\n",
    "\n",
    "\n",
    "    # Block 4: input is 16 x 16 x (64 * 2)\n",
    "    model.add(Conv2DTranspose(64 * 1, kernel_size=4, strides= 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_transpose_4'))\n",
    "    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8,  center=1.0, scale=0.02, name='bn_4'))\n",
    "    model.add(ReLU(name='relu_4'))\n",
    "\n",
    "    model.add(Conv2DTranspose(3, 4, 2,padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, \n",
    "                              activation='tanh', name='conv_transpose_5'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = make_generator()\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the Discriminator\n",
    "\n",
    "#The Discriminator has five convolution layers.\n",
    "#All but the first and final Conv2D layers have Batch Normalization, \n",
    "#since directly applying batchnorm to all layers could result in sample oscillation \n",
    "#and model instability; The first four Conv2D layers use the Leaky-Relu activation \n",
    "#with a slope of 0.2. Lastly, instead of a fully connected layer, the output layer \n",
    "#has a convolution layer with a Sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator():\n",
    "    \n",
    "    model=Sequential()\n",
    "    \n",
    "    # Block 1: input is 64 x 64 x (3)\n",
    "    model.add(Input(shape=(64, 64, 3), name='input_layer'))\n",
    "    model.add(Conv2D(64, kernel_size=4, strides= 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_1'))\n",
    "    model.add(LeakyReLU(0.2, name='leaky_relu_1'))\n",
    "\n",
    "    # Block 2: input is 32 x 32 x (64)\n",
    "    model.add(Conv2D(64 * 2, kernel_size=4, strides= 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_2'))\n",
    "    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_1'))\n",
    "    model.add(LeakyReLU(0.2, name='leaky_relu_2'))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(Conv2D(64 * 4, 4, 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_3'))\n",
    "    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_2'))\n",
    "    model.add(LeakyReLU(0.2, name='leaky_relu_3'))\n",
    "\n",
    "\n",
    "    #Block 4\n",
    "    model.add(Conv2D(64 * 8, 4, 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False, name='conv_4'))\n",
    "    model.add(BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_3'))\n",
    "    model.add(LeakyReLU(0.2, name='leaky_relu_4'))\n",
    "\n",
    "\n",
    "    #Block 5\n",
    "    model.add(Conv2D(1, 4, 2,padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), use_bias=False,  \n",
    "                     activation='sigmoid', name='conv_5'))\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = make_discriminator()\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Loss Functions\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(Xhat):\n",
    "    return cross_entropy(tf.ones_like(Xhat), Xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(X, Xhat):\n",
    "    real_loss = cross_entropy(tf.ones_like(X), X)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(Xhat), Xhat)\n",
    "    total_loss = 0.5*(real_loss + fake_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Optimizers\n",
    "earning_rate = 0.0002\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(lr = 0.0002, beta_1 = 0.5, beta_2 = 0.999 )\n",
    "\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(lr = 0.0002, beta_1 = 0.5, beta_2 = 0.999 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "epochs=1\n",
    "\n",
    "discriminator=make_discriminator()\n",
    "\n",
    "generator= make_generator()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #data for the true distribution of your real data samples training ste\n",
    "    start = time.time()\n",
    "    i=0\n",
    "    for X in tqdm(normalized_ds, desc=f\"epoch {epoch+1}\", total=len(normalized_ds)):\n",
    "        \n",
    "        i+=1\n",
    "        if i%1000:\n",
    "            print(\"epoch {}, iteration {}\".format(epoch+1, i))\n",
    "            \n",
    "        train_step(X)\n",
    "    \n",
    "\n",
    "    noise = tf.random.normal([BATCH_SIZE, 1, 1, latent_dim])\n",
    "    Xhat=generator(noise,training=False)\n",
    "    X=[x for x in normalized_ds]\n",
    "    print(\"orignal images\")\n",
    "    plot_array(X[0])\n",
    "    print(\"generated images\")\n",
    "    plot_array(Xhat)\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_url=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module6/generator.tar.gz\"\n",
    "await skillsnetwork.prepare(generator_url, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "full_generator=load_model(\"generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim=100\n",
    "\n",
    "# input consists of noise vectors\n",
    "noise = tf.random.normal([200, 1, 1, latent_dim])\n",
    "\n",
    "# feed the noise vectors to the generator\n",
    "Xhat=full_generator(noise,training=False)\n",
    "plot_array(Xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
